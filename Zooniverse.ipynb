{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ZOONIVERSE DROP-DOWN TEXT REDUCER AND CARD COMBINER\n",
    "This Jupyter notebook contains basic code to process Zooniverse data beyond the Panoptes GUI provided by Zooniverse.\n",
    "\n",
    "As of the time this was written (December, 2020), the \"Data Exports\" page on Zooniverse allows you to download a Panoptes Aggregation GUI. This GUI creates a reduced consensus file for both drop-downs and text. However, the reduced drop-down file is currently hashed and not fully reduced. This code attempts to complete those final steps by unhashing the drop-down file and combining the text and drop-downs.\n",
    "\n",
    "### Inputs\n",
    "This takes as input the files paths for the csvs...\n",
    "- workflows (from Zooniverse data export)\n",
    "- reduced drop-downs (from Panoptes)\n",
    "- reduced text (from Panoptes)\n",
    "- where you would like to output an unhashed reduced drop-down csv\n",
    "- where you would like to output a combined reduced drop-down (unhashed) and free text csv\n",
    "\n",
    "### Outputs\n",
    "This outputs csvs...\n",
    "- \"dropdown_max_reduced.csv\" unhashed reduced drop-down csv\n",
    "- \"final.csv\" combined reduced drop-down (unhashed) and free text csv\n",
    "\n",
    "### Process\n",
    "It does so by using the workflows csv to create a nested dictionary (workflow --> task --> hash --> text) to unhash the reduced drop-down text\n",
    "It then unhashes the reduced drop-down csv and writes this data to a new max reduced file\n",
    "It then combines the max reduced drop-down text and reduced text into a new file\n",
    "\n",
    "### Known Code Flaws\n",
    "Please be advised this code is rustic and contains several potential flaws...\n",
    "- it has not been factored out into highly re-usable functions\n",
    "- it does not preserve the order of the headers when it combines the csv files, so the user must re-order them in Excel manually\n",
    "- it iterates through the csv files and data structures in their entirety (this could become a problem with larger volumes of data)\n",
    "- it makes some assumptions about the structure of the Zooniverse data such as the fact that the workflows csv is ordered from oldest to newest version, and it hard-codes the header names it uses to access data (ex. \"workflow_id\")\n",
    "\n",
    "### Questions?\n",
    "If you have questions about this code's origin or purpose, please contact Library-RDS@northeastern.edu\n",
    "\n",
    "For supporting documentation on the Panoptes GUI, please see https://aggregation-caesar.zooniverse.org/index.html\n",
    "\n",
    "For a friendly \"idiot's guide\" explanation to the back-end of Zooniverse, please see https://www.zooniverse.org/talk/18/1439900\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: Upload Files\n",
    "There are 3 CSV files that you will need to upload:\n",
    "- workflows (from Zooniverse data export, ex. \"boston-phoenix-1974-workflows.csv\")\n",
    "- reduced drop-downs (from Panoptes GUI output, ex. \"dropdown_reducer_reductions.csv\")\n",
    "- reduced text (from Panoptes GUI output, ex. \"text_reducer_reductions.csv\")\n",
    "\n",
    "To upload your files, do the following:\n",
    "\n",
    "<ol>\n",
    "    <li style=\"clear:both\">Go to the main menu for this project by clicking on the \"Jupyter\" icon in the upper left-hand corner of your screen.<br></br><img src=\"https://raw.githubusercontent.com/KNortheastern/Zooniverse/master/logo.png\" style=\"float:left\"></li>\n",
    "    <li style=\"clear:both\">Click the \"Upload\" button on the right-hand side of the screen and, when prompted, select the file you want to upload.<br></br><img src=\"https://raw.githubusercontent.com/KNortheastern/Zooniverse/master/Screenshots2.png\" style=\"float:left\" width=\"650\"></li>\n",
    "    <li style=\"clear:both\">When the file is ready to be uploaded, it will appear on the page with another \"Upload\" button to confirm the upload. Click the \"Upload\" button to complete the uploading of this file.<br></br><img src=\"https://raw.githubusercontent.com/KNortheastern/Zooniverse/master/Screenshots4.png\" style=\"float:left\" width=\"650\"></li>\n",
    "    <li style=\"clear:both\">Repeat this upload process for all 3 CSVs. At the end, your screen should look something like this.<br></br><img src=\"https://raw.githubusercontent.com/KNortheastern/Zooniverse/master/Screenshots6.png\" style=\"float:left\" width=\"650\"></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: Update file names if necessary\n",
    "Please check the file names in the code cell below. If necessary, update the file names to match the ones that you are using. When finished checking and updating, run the code cell.\n",
    "- csvPathWorkflows contains the name of the workflows CSV\n",
    "- csvPathReduced contains the name of the reduced dropdown CSV\n",
    "- csvTextRed contains the name of the reduced text CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify file paths for CSV inputs\n",
    "csvPathWorkflows = \"./boston-phoenix-1974-workflows.csv\"\n",
    "csvPathReduced = \"./dropdown_reducer_reductions.csv\"\n",
    "csvTextRed = \"./text_reducer_reductions.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: Run code\n",
    "Run each of the code cells in this step, in order. The steps are broken out with short explanations in case changes need to be made in the future or errors occur and there is a need for troubleshooting.\n",
    "### STEP 3a: set up the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import any additional libraries we need\n",
    "import csv # to read CSVs\n",
    "import json # to read JSONs\n",
    "\n",
    "# specify file paths for CSV outputs\n",
    "csvPathMaxRed = \"./dropdown_max_reduced.csv\"\n",
    "csvFinal = \"./final.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3b: read the workflows csv file and create a nested dictionary to unhash the drop-down data\n",
    "Note: the dictionary will be used to look up each hash by workflow --> task --> hash --> text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty dictionary to hold the outer level indexed by workflow (workflow --> task --> hash --> text)\n",
    "workflowDict = {}\n",
    "\n",
    "# read the workflows csv file and populate the dictionary with an entry for each workflow\n",
    "with open(csvPathWorkflows, 'r') as csvFile:\n",
    "    csvReader = csv.DictReader(csvFile)\n",
    "\n",
    "    # for each row in the csv file...\n",
    "    for eachRow in csvReader:\n",
    "        # Note: eachRow is a dict with the following keys (can see using eachRow.keys() )\n",
    "            #['tutorial_subject_id',\n",
    "            # 'tasks',\n",
    "            # 'display_name',\n",
    "            # 'prioritized',\n",
    "            # 'first_task',\n",
    "            # 'aggregation',\n",
    "            # 'classifications_count',\n",
    "            # 'minor_version',\n",
    "            # 'grouped',\n",
    "            # 'workflow_id',\n",
    "            # 'version',\n",
    "            # 'pairwise',\n",
    "            # 'retirement',\n",
    "            # 'primary_language',\n",
    "            # 'active',\n",
    "            # 'retired_set_member_subjects_count',\n",
    "            # 'strings']\n",
    "            \n",
    "        # create a unique ID for the workflow using \"workflow_id\"\n",
    "        # note: the file also contains \"version\" and \"minor_version\" data, but this is safely ignored\n",
    "        #       because the csv is written in order from oldest to newest such that iterating through\n",
    "        #       the csv file overwrites (and therefore updates) the older entries\n",
    "        workflowId = eachRow['workflow_id']\n",
    "        uniqueID = workflowId\n",
    "\n",
    "        # create an empty dictionary to hold the second level dictionary indexed by task (task --> hash --> text)\n",
    "        workflowTaskDict = {}\n",
    "\n",
    "        # grab the tasks and strings columns as JSONs\n",
    "        tasksDict = json.loads(eachRow['tasks']) # contains hash --> label (ex. \"T1.selects.0.options.*.0.label\")\n",
    "        stringsDict = json.loads(eachRow['strings']) # contains label --> text (ex. \"none\")\n",
    "\n",
    "        # iterate through the tasks and for each task, populate the dictionary by...\n",
    "        for key,val in list(tasksDict.items()):\n",
    "            # create an empty dictionary to hold third level dictionary indexed by hash (hash --> text)\n",
    "            workflowHashDict = {}\n",
    "\n",
    "            # try to do the following...\n",
    "            try:\n",
    "                # grab the list containing pairs of hashes (ex. \"5bd2fbcf507d4\") and labels (ex. \"T1.selects.0.options.*.0.label\")\n",
    "                hashList = val['selects'][0]['options']['*']\n",
    "\n",
    "                # iterate through each pair in the hashlist and...\n",
    "                for eachDict in hashList:\n",
    "\n",
    "                    # grab the hash and label\n",
    "                    hashValue = str(eachDict['value'])\n",
    "                    hashLabel = eachDict['label']\n",
    "\n",
    "                    # look the label up in the strings dictionary\n",
    "                    hashString = stringsDict[hashLabel]\n",
    "\n",
    "                    # create a new dictionary entry for this hash\n",
    "                    workflowHashDict[hashValue] = hashString\n",
    "\n",
    "                # create a new dictionary entry in the workflow for this task\n",
    "                workflowTaskDict[key] = workflowHashDict\n",
    "\n",
    "            # if an error while trying, do nothing...\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "        # create a new entry in the overall workflow dictionary for all of this workflow's tasks\n",
    "        workflowDict[uniqueID] = workflowTaskDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3c: use the hash dictionary to unhash the reduced drop-downs csv file and write a new max reduced file\n",
    "Note: This step outputs the \"dropdown_max_reduced.csv\" CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the reduced dropdowns csv file\n",
    "with open(csvPathReduced, 'r') as csvFile:\n",
    "    csvReader = csv.DictReader(csvFile)\n",
    "\n",
    "    # start the process of writing the new file\n",
    "    csvHeaders = list(next(csvReader).keys())\n",
    "    writer = csv.writer(open(csvPathMaxRed, 'w'))\n",
    "    csvHeaders.append(\"Unhashed\") # add column header for new unhashed data\n",
    "    writer.writerow(csvHeaders)\n",
    "\n",
    "    # for each row in the reduced drop-downs csv file...\n",
    "    for eachRow in csvReader:\n",
    "        # grab the workflow id, task, and list of hashes\n",
    "        workflowId = eachRow[\"workflow_id\"]\n",
    "        taskID = eachRow[\"task\"]\n",
    "        hashDict = eval(eachRow[\"data.value\"])[0] # ex. \"{'None': 1, 'cd638da74b4e3': 1, 'e4d9c81e23fe3': 5}\"\n",
    "        numHash = len(hashDict)\n",
    "\n",
    "        # assume there is no unhashed answer until you can get an answer\n",
    "        unHashed = \"\"\n",
    "\n",
    "        # for each hash...\n",
    "        for key,val in list(hashDict.items()):\n",
    "\n",
    "            # try to look up the hash in the workflow (workflow --> task --> hash --> text) dictionary...\n",
    "            try:\n",
    "                # if there is only one hash voted on, make the unhashed answer that answer\n",
    "                if numHash == 1:\n",
    "                    unHashed = str(workflowDict[workflowId][taskID][key])\n",
    "                # otherwise, make a long string with all of the answers and the votes they received\n",
    "                else:\n",
    "                    unHashed = unHashed + \" [\" + str(val) + \" votes] \" + str(workflowDict[workflowId][taskID][key])\n",
    "\n",
    "            # if the hash wasn't in the dictionary, handle it as free-text...\n",
    "            except KeyError:\n",
    "                # if there is only one free-text voted on, make the unhashed answer that answer\n",
    "                if numHash == 1:\n",
    "                    unHashed = str(key)\n",
    "                # otherwise, add the free text-to the string of answers along with the number of votes it received\n",
    "                else:\n",
    "                    unHashed = unHashed + \" [\" + str(val) + \" votes] \" + str(key)\n",
    "                continue\n",
    "                \n",
    "        # add the unhashed data to this row as though it had been there all along\n",
    "        eachRow[\"Unhashed\"] = unHashed\n",
    "\n",
    "        # write the row (now with unhashed data) to the new file\n",
    "        writer.writerow(tuple(eachRow.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3d: combine the new max reduced drop-down file with the reduced text file\n",
    "Note: This step outputs the \"final.csv\" CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure out combined headers for the new combined csv file\n",
    "# assume there are no headers at first\n",
    "combinedHeaders = []\n",
    "\n",
    "\n",
    "# find the headers from the max reduced drop-down csv file\n",
    "with open(csvPathMaxRed, 'r') as csvFile:\n",
    "    csvReader = csv.DictReader(csvFile)\n",
    "    csvPathMaxRedHeaders = list(next(csvReader).keys())\n",
    "\n",
    "# find the headers from the reduced text csv file\n",
    "with open(csvTextRed, 'r') as csvFile:\n",
    "    csvReader = csv.DictReader(csvFile)\n",
    "    csvTextRedHeaders = list(next(csvReader).keys())\n",
    "\n",
    "# combine the two sets of headers\n",
    "combinedHeaders = list(set(csvPathMaxRedHeaders + csvTextRedHeaders))\n",
    "\n",
    "\n",
    "# figure out combined data for the new combined csv file\n",
    "# add the data from the max reduced drop-down csv file\n",
    "csvPathMaxRedData = []\n",
    "with open(csvPathMaxRed, 'r') as csvFile:\n",
    "    csvReader = csv.DictReader(csvFile)\n",
    "\n",
    "    for eachRow in csvReader:\n",
    "\n",
    "        # create a blank dictionary with all of the combined headers\n",
    "        rowOfDataDict = {}\n",
    "        for eachKey in combinedHeaders:\n",
    "            rowOfDataDict[eachKey] = \"\"\n",
    "\n",
    "        # populate as much of this dictionary as we can (i.e. some headers won't be relevant to this csv)\n",
    "        for key,val in list(eachRow.items()):\n",
    "            rowOfDataDict[key] = val\n",
    "\n",
    "        csvPathMaxRedData.append(rowOfDataDict)\n",
    "\n",
    "# add the data from the reduced text csv file\n",
    "csvTextRedData = []\n",
    "with open(csvTextRed, 'r') as csvFile:\n",
    "    csvReader = csv.DictReader(csvFile)\n",
    "    for eachRow in csvReader:\n",
    "\n",
    "        # create a blank dictionary with all of the combined headers\n",
    "        rowOfDataDict = {}\n",
    "        for eachKey in combinedHeaders:\n",
    "            rowOfDataDict[eachKey] = \"\"\n",
    "\n",
    "        # populate as much of this dictionary as we can (i.e. some headers won't be relevant to this csv)\n",
    "        for key,val in list(eachRow.items()):\n",
    "            rowOfDataDict[key] = val\n",
    "\n",
    "        csvTextRedData.append(rowOfDataDict)\n",
    "\n",
    "# combine the data from both csvs\n",
    "combinedData = csvPathMaxRedData + csvTextRedData\n",
    "\n",
    "\n",
    "# write the combined data to the new combined csv file\n",
    "with open(csvFinal, 'w') as csvfile:  \n",
    "        writer = csv.DictWriter(csvfile, fieldnames=combinedHeaders)\n",
    "        writer.writeheader()\n",
    "        for data in csvPathMaxRedData:\n",
    "            writer.writerow(data)\n",
    "\n",
    "        for data in csvTextRedData:\n",
    "            writer.writerow(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4: Download output\n",
    "Once the code has finished running, you should see two new CSV files in your main menu:\n",
    "- \"dropdown_max_reduced.csv\" is an intermediate CSV file and contains the unhashed drop-downs\n",
    "- \"final.csv\" is the final CSV file and contains the fully reduced and combined data for both the text and drop-downs\n",
    "\n",
    "Download the \"final.csv\" file.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/KNortheastern/Zooniverse/master/Screenshots7.png\" align=\"left\" width=\"650\">\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
