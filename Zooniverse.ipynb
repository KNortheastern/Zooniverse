{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================\n",
    "# ZOONIVERSE DROP-DOWN TEXT REDUCER AND CARD COMBINER\n",
    "# =================================================================\n",
    "# Note: This file contains basic code to process Zooniverse data beyond the Panoptes GUI\n",
    "#\n",
    "#\t\tAs of the time this was written (December 18, 2020), the \"Data Exports\" page on Zooniverse allows you to download\n",
    "#\t\ta Panoptes Aggregation GUI. This GUI creates a reduced consensus file for both drop-downs and text.\n",
    "#\t\tHowever, the reduced drop-down file is currently hashed and not fully reduced.\n",
    "#\t\tThis code attempts to complete those final steps by unhashing the drop-down file and combining the text and drop-downs.\n",
    "#\n",
    "#       This takes as input the files paths for the csvs...\n",
    "#       \t-workflows (from Panoptes)\n",
    "#       \t-reduced drop-downs (from Panoptes)\n",
    "#       \t-reduced text (from Panoptes)\n",
    "#       \t-where you would like to output an unhashed reduced drop-down csv\n",
    "#       \t-where you would like to output a combined reduced drop-down (unhashed) and free text csv\n",
    "#\n",
    "#       This outputs csvs...\n",
    "#       \t-unhashed reduced drop-down csv\n",
    "#       \t-combined reduced drop-down (unhashed) and free text csv\n",
    "#\n",
    "#\t\tIt does so by using the workflows csv to create a nested dictionary (workflow --> task --> hash --> text) to unhash the reduced drop-down text\n",
    "#\t\tIt then unhashes the reduced drop-down csv and writes this data to a new max reduced file\n",
    "#\t\tIt then combines the max reduced drop-down text and reduced text into a new file\n",
    "#\n",
    "#       Please be advised this code is rustic and contains several potential flaws...\n",
    "#       \t-it has not been factored out into highly re-usable functions\n",
    "#       \t-it does not preserve the order of the headers when it combines the csv files, so the user must re-order them in Excel manually\n",
    "#       \t-it iterates through the csv files and data structures in their entirety (this could become a problem with larger volumes of data)\n",
    "#       \t-it makes some assumptions about the structure of the Zooniverse data such as the fact that the workflows csv is ordered from oldest to newest version,\n",
    "#\t\t\t and it hard-codes the header names it uses to access data (ex. \"workflow_id\")\n",
    "#\n",
    "#       If you have questions about this code's origin or purpose, please contact Library-RDS@northeastern.edu\n",
    "#\t\tFor supporting documentation on the Panoptes GUI, please see https://aggregation-caesar.zooniverse.org/index.html\n",
    "#\t\tFor a friendly \"idiot's guide\" explanation to the back-end of Zooniverse, please see https://www.zooniverse.org/talk/18/1439900\n",
    "\n",
    "\n",
    "# =================================================================\n",
    "# STEP #1: set up the task (please note that user input is required for file paths)\n",
    "# import any additional libraries we need\n",
    "import csv # to read CSVs\n",
    "import json # to read JSONs\n",
    "\n",
    "# specify file paths for CSVs\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# USER MUST CHANGE FILE PATHS BELOW FOR CODE TO WORK\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# inputs\n",
    "csvPathWorkflows = \"./boston-phoenix-1974-workflows.csv\"\n",
    "csvPathReduced = \"./dropdown_reducer_reductions.csv\"\n",
    "csvTextRed = \"./text_reducer_reductions.csv\"\n",
    "# outputs\n",
    "csvPathMaxRed = \"./dropdown_max_reduced.csv\"\n",
    "csvFinal = \"./final.csv\"\n",
    "\n",
    "# =================================================================\n",
    "# STEP 2: read the workflows csv file and create a nested dictionary to unhash the drop-down data\n",
    "# note: the dictionary will be used to look up each hash by workflow --> task --> hash --> text\n",
    "\n",
    "# create an empty dictionary to hold the outer level indexed by workflow (workflow --> task --> hash --> text)\n",
    "workflowDict = {}\n",
    "\n",
    "# read the workflows csv file and populate the dictionary with an entry for each workflow\n",
    "with open(csvPathWorkflows, 'r') as csvFile:\n",
    "\tcsvReader = csv.DictReader(csvFile)\n",
    "\t\n",
    "\t# for each row in the csv file...\n",
    "\tfor eachRow in csvReader:\n",
    "\t\t# Note: eachRow is a dict with the following keys (can see using eachRow.keys() )\n",
    "\t\t\t#['tutorial_subject_id',\n",
    "\t\t\t# 'tasks',\n",
    "\t\t\t# 'display_name',\n",
    "\t\t\t# 'prioritized',\n",
    "\t\t\t# 'first_task',\n",
    "\t\t\t# 'aggregation',\n",
    "\t\t\t# 'classifications_count',\n",
    "\t\t\t# 'minor_version',\n",
    "\t\t\t# 'grouped',\n",
    "\t\t\t# 'workflow_id',\n",
    "\t\t\t# 'version',\n",
    "\t\t\t# 'pairwise',\n",
    "\t\t\t# 'retirement',\n",
    "\t\t\t# 'primary_language',\n",
    "\t\t\t# 'active',\n",
    "\t\t\t# 'retired_set_member_subjects_count',\n",
    "\t\t\t# 'strings']\n",
    "\t\t\n",
    "\t\t# create a unique ID for the workflow using \"workflow_id\"\n",
    "\t\t# note: the file also contains \"version\" and \"minor_version\" data, but this is safely ignored\n",
    "\t\t#       because the csv is written in order from oldest to newest such that iterating through\n",
    "\t\t#       the csv file overwrites (and therefore updates) the older entries\n",
    "\t\tworkflowId = eachRow['workflow_id']\n",
    "\t\tuniqueID = workflowId\n",
    "\n",
    "\t\t# create an empty dictionary to hold the second level dictionary indexed by task (task --> hash --> text)\n",
    "\t\tworkflowTaskDict = {}\n",
    "\n",
    "\t\t# grab the tasks and strings columns as JSONs\n",
    "\t\ttasksDict = json.loads(eachRow['tasks']) # contains hash --> label (ex. \"T1.selects.0.options.*.0.label\")\n",
    "\t\tstringsDict = json.loads(eachRow['strings']) # contains label --> text (ex. \"none\")\n",
    "\t\n",
    "\t\t# iterate through the tasks and for each task, populate the dictionary by...\n",
    "\t\tfor key,val in list(tasksDict.items()):\n",
    "\t\t\t# create an empty dictionary to hold third level dictionary indexed by hash (hash --> text)\n",
    "\t\t\tworkflowHashDict = {}\n",
    "\n",
    "\t\t\t# try to do the following...\n",
    "\t\t\ttry:\n",
    "\t\t\t\t# grab the list containing pairs of hashes (ex. \"5bd2fbcf507d4\") and labels (ex. \"T1.selects.0.options.*.0.label\")\n",
    "\t\t\t\thashList = val['selects'][0]['options']['*']\n",
    "\n",
    "\t\t\t\t# iterate through each pair in the hashlist and...\n",
    "\t\t\t\tfor eachDict in hashList:\n",
    "\n",
    "\t\t\t\t\t# grab the hash and label\n",
    "\t\t\t\t\thashValue = str(eachDict['value'])\n",
    "\t\t\t\t\thashLabel = eachDict['label']\n",
    "\n",
    "\t\t\t\t\t# look the label up in the strings dictionary\n",
    "\t\t\t\t\thashString = stringsDict[hashLabel]\n",
    "\n",
    "\t\t\t\t\t# create a new dictionary entry for this hash\n",
    "\t\t\t\t\tworkflowHashDict[hashValue] = hashString\n",
    "\n",
    "\t\t\t\t# create a new dictionary entry in the workflow for this task\n",
    "\t\t\t\tworkflowTaskDict[key] = workflowHashDict\n",
    "\n",
    "\t\t\t# if an error while trying, do nothing...\n",
    "\t\t\texcept KeyError:\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t# create a new entry in the overall workflow dictionary for all of this workflow's tasks\n",
    "\t\tworkflowDict[uniqueID] = workflowTaskDict\n",
    "\n",
    "\n",
    "\n",
    "# =================================================================\n",
    "# STEP 3: use the hash dictionary to unhash the reduced drop-downs csv file and write a new max reduced file\n",
    "\n",
    "# read the reduced dropdowns csv file\n",
    "with open(csvPathReduced, 'r') as csvFile:\t\n",
    "\tcsvReader = csv.DictReader(csvFile)\n",
    "\t\n",
    "\t# start the process of writing the new file\n",
    "\tcsvHeaders = list(next(csvReader).keys())\n",
    "\twriter = csv.writer(open(csvPathMaxRed, 'w'))\n",
    "\tcsvHeaders.append(\"Unhashed\") # add column header for new unhashed data\n",
    "\twriter.writerow(csvHeaders)\n",
    "\n",
    "\t# for each row in the reduced drop-downs csv file...\n",
    "\tfor eachRow in csvReader:\n",
    "\t\t# grab the workflow id, task, and list of hashes\n",
    "\t\tworkflowId = eachRow[\"workflow_id\"]\n",
    "\t\ttaskID = eachRow[\"task\"]\n",
    "\t\thashDict = eval(eachRow[\"data.value\"])[0] # ex. \"{'None': 1, 'cd638da74b4e3': 1, 'e4d9c81e23fe3': 5}\"\n",
    "\t\tnumHash = len(hashDict)\n",
    "\n",
    "\t\t# assume there is no unhashed answer until you can get an answer\n",
    "\t\tunHashed = \"\"\n",
    "\n",
    "\t\t# for each hash...\n",
    "\t\tfor key,val in list(hashDict.items()):\n",
    "\n",
    "\t\t\t# try to look up the hash in the workflow (workflow --> task --> hash --> text) dictionary...\n",
    "\t\t\ttry:\n",
    "\t\t\t\t# if there is only one hash voted on, make the unhashed answer that answer\n",
    "\t\t\t\tif numHash == 1:\n",
    "\t\t\t\t\tunHashed = str(workflowDict[workflowId][taskID][key])\n",
    "\t\t\t\t# otherwise, make a long string with all of the answers and the votes they received\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tunHashed = unHashed + \" [\" + str(val) + \" votes] \" + str(workflowDict[workflowId][taskID][key])\n",
    "\t\t\t\n",
    "\t\t\t# if the hash wasn't in the dictionary, handle it as free-text...\n",
    "\t\t\texcept KeyError:\n",
    "\t\t\t\t# if there is only one free-text voted on, make the unhashed answer that answer\n",
    "\t\t\t\tif numHash == 1:\n",
    "\t\t\t\t\tunHashed = str(key)\n",
    "\t\t\t\t# otherwise, add the free text-to the string of answers along with the number of votes it received\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tunHashed = unHashed + \" [\" + str(val) + \" votes] \" + str(key)\n",
    "\t\t\t\tcontinue\t\t\n",
    "\t\n",
    "\t\t# add the unhashed data to this row as though it had been there all along\n",
    "\t\teachRow[\"Unhashed\"] = unHashed\n",
    "\n",
    "\t\t# write the row (now with unhashed data) to the new file\n",
    "\t\twriter.writerow(tuple(eachRow.values()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =================================================================\n",
    "# STEP 4: combine the new max reduced drop-down file with the reduced text file\n",
    "\n",
    "# figure out combined headers for the new combined csv file\n",
    "# assume there are no headers at first\n",
    "combinedHeaders = []\n",
    "\n",
    "\n",
    "# find the headers from the max reduced drop-down csv file\n",
    "with open(csvPathMaxRed, 'r') as csvFile:\t\n",
    "\tcsvReader = csv.DictReader(csvFile)\n",
    "\tcsvPathMaxRedHeaders = list(next(csvReader).keys())\n",
    "\n",
    "# find the headers from the reduced text csv file\n",
    "with open(csvTextRed, 'r') as csvFile:\t\n",
    "\tcsvReader = csv.DictReader(csvFile)\n",
    "\tcsvTextRedHeaders = list(next(csvReader).keys())\n",
    "\n",
    "# combine the two sets of headers\n",
    "combinedHeaders = list(set(csvPathMaxRedHeaders + csvTextRedHeaders))\n",
    "\n",
    "\n",
    "# figure out combined data for the new combined csv file\n",
    "# add the data from the max reduced drop-down csv file\n",
    "csvPathMaxRedData = []\n",
    "with open(csvPathMaxRed, 'r') as csvFile:\t\n",
    "\tcsvReader = csv.DictReader(csvFile)\n",
    "\n",
    "\tfor eachRow in csvReader:\n",
    "\n",
    "\t\t# create a blank dictionary with all of the combined headers\n",
    "\t\trowOfDataDict = {}\n",
    "\t\tfor eachKey in combinedHeaders:\n",
    "\t\t\trowOfDataDict[eachKey] = \"\"\n",
    "\n",
    "\t\t# populate as much of this dictionary as we can (i.e. some headers won't be relevant to this csv)\n",
    "\t\tfor key,val in list(eachRow.items()):\n",
    "\t\t\trowOfDataDict[key] = val\n",
    "\n",
    "\t\tcsvPathMaxRedData.append(rowOfDataDict)\n",
    "\n",
    "# add the data from the reduced text csv file\n",
    "csvTextRedData = []\n",
    "with open(csvTextRed, 'r') as csvFile:\t\n",
    "\tcsvReader = csv.DictReader(csvFile)\n",
    "\tfor eachRow in csvReader:\n",
    "\n",
    "\t\t# create a blank dictionary with all of the combined headers\n",
    "\t\trowOfDataDict = {}\n",
    "\t\tfor eachKey in combinedHeaders:\n",
    "\t\t\trowOfDataDict[eachKey] = \"\"\n",
    "\n",
    "\t\t# populate as much of this dictionary as we can (i.e. some headers won't be relevant to this csv)\n",
    "\t\tfor key,val in list(eachRow.items()):\n",
    "\t\t\trowOfDataDict[key] = val\n",
    "\n",
    "\t\tcsvTextRedData.append(rowOfDataDict)\n",
    "\n",
    "# combine the data from both csvs\n",
    "combinedData = csvPathMaxRedData + csvTextRedData\n",
    "\n",
    "\n",
    "# write the combined data to the new combined csv file\n",
    "with open(csvFinal, 'w') as csvfile:  \n",
    "        writer = csv.DictWriter(csvfile, fieldnames=combinedHeaders)\n",
    "        writer.writeheader()\n",
    "        for data in csvPathMaxRedData:\n",
    "            writer.writerow(data)\n",
    "\n",
    "        for data in csvTextRedData:\n",
    "            writer.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
